# Copyright 2025 Cisco Systems, Inc. and its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

import logging
import uuid

from langchain_core.messages import AIMessage
from langgraph.graph.state import CompiledStateGraph
from langgraph.prebuilt import create_react_agent
from langgraph_supervisor import create_supervisor

from common.llm import get_llm
from graph.tools import FlavorProfileTool

from farm.card import AGENT_CARD as farm_agent_card

logger = logging.getLogger("corto.supervisor.graph")


class ExchangeGraph:
    def __init__(self):
        self.graph = self.build_graph()

    def build_graph(self) -> CompiledStateGraph:
        """
        Constructs and compiles a LangGraph instance.

        This function initializes a `SupervisorAgent` to create the base graph structure
        and uses an `InMemorySaver` as the checkpointer for the compilation process.

        The resulting compiled graph can be used to execute Supervisor workflow in LangGraph Studio.

        Returns:
        CompiledGraph: A fully compiled LangGraph instance ready for execution.
        """
        model = get_llm()

        # initialize the flavor profile tool(used for coffee flavor, taste, or sensory profile estimation) with the farm agent card
        flavor_profile_tool = FlavorProfileTool(
            remote_agent_card=farm_agent_card,
        )

        #  worker agent- always responsible for flavor, taste, or sensory profile of coffee queries
        get_flavor_profile_a2a_agent = create_react_agent(
            model=model,
            tools=[flavor_profile_tool],  # list of tools for the agent
            name="get_flavor_profile_via_a2a",
        )
        graph = create_supervisor(
            model=model,
            agents=[get_flavor_profile_a2a_agent],  # worker agents list
            prompt=(
                "You are a supervisor agent\n"
                "For any user prompt related to the flavor, taste, or sensory profile of coffee, always assign the task to the worker agent 'get_flavor_profile_via_a2a' and ensure the associated tool - `flavor_profile_tool` is always used.\n"
                "If the user's prompt is not about coffee flavor, taste, or sensory profile, respond: "
                "\"I'm sorry, I cannot assist with that request. Please ask about coffee flavor or taste.\"\n"
                "If the worker agent returns control to you and it is a success and does not contain errors, do not generate any further messages or responses. End the conversation immediately by returning an empty response.\n"
                "If the worker agent returns control to you and it is an error, return the same error message.\n"
            ),
            add_handoff_back_messages=False,
            output_mode="last_message",
        ).compile()
        logger.debug("LangGraph supervisor created and compiled successfully.")
        return graph

    async def serve(self, prompt: str):
        """
        Processes the input prompt and returns a response from the graph.
        Args:
            prompt (str): The input prompt to be processed by the graph.
        Returns:
            str: The response generated by the graph based on the input prompt.
        """
        try:
            logger.debug(f"Received prompt: {prompt}")
            if not isinstance(prompt, str) or not prompt.strip():
                raise ValueError("Prompt must be a non-empty string.")
            result = await self.graph.ainvoke({
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
            }, {"configurable": {"thread_id": uuid.uuid4()}})

            messages = result.get("messages", [])
            if not messages:
                raise RuntimeError("No messages found in the graph response.")

            # Find the last AIMessage with non-empty content
            for message in reversed(messages):
                if isinstance(message, AIMessage) and message.content.strip():
                    logger.debug(f"Valid AIMessage found: {message.content.strip()}")
                    return message.content.strip()

            raise RuntimeError("No valid AIMessage found in the graph response.")
        except ValueError as ve:
            logger.error(f"ValueError in serve method: {ve}")
            raise ValueError(str(ve))
        except Exception as e:
            logger.error(f"Error in serve method: {e}")
            raise Exception(str(e))
