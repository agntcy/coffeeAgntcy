PYTHONPATH=.

# Recommended temperature setting for OpenAI models
OPENAI_TEMPERATURE=0.7

# By default, the latest cnoe-agent-utils enables streaming via this env var
LLM_STREAMING=False

# === OpenAI Settings ===
LLM_PROVIDER=openai
OPENAI_API_KEY="your_openai_api_key_here"
OPENAI_ENDPOINT=https://api.openai.com/v1 # Default OpenAI endpoint without proxy
OPENAI_MODEL_NAME=gpt-4o

# === Azure OpenAI Settings ===
# Uncomment and fill in to use Azure instead of OpenAI
# LLM_PROVIDER=azure-openai
# AZURE_OPENAI_ENDPOINT=https://your-azure-resource.openai.azure.com/
# AZURE_OPENAI_DEPLOYMENT=gpt-4-prod
# AZURE_OPENAI_API_KEY=your_azure_api_key
# AZURE_OPENAI_API_VERSION=2023-12-01-preview`

#============================
# Local Development Settings
#============================
#(not needed when running Docker Compose)

# OTEL environment variables
# OTLP_HTTP_ENDPOINT="http://localhost:4318"

# === Transport Settings ===
# SLIM (Default):
# DEFAULT_MESSAGE_TRANSPORT=SLIM
# TRANSPORT_SERVER_ENDPOINT=http://localhost:46357

# Alternative: NATS transport (uncomment to use NATS). The endpoint address must be in the form: nats://host:port.
# DEFAULT_MESSAGE_TRANSPORT=NATS
# TRANSPORT_SERVER_ENDPOINT=nats://localhost:4222
