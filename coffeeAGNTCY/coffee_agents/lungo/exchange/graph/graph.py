# Copyright 2025 Cisco Systems, Inc. and its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

import logging
import uuid

from langchain_core.messages import AIMessage

from langgraph.graph.state import CompiledStateGraph
from langgraph.prebuilt import create_react_agent
from langgraph_supervisor import create_supervisor

from common.llm import get_llm
from farms.brazil.card import AGENT_CARD as farm_agent_card
from graph.tools import FetchHarvestTool, GetFarmYieldTool

logger = logging.getLogger("corto.supervisor.graph")

class ExchangeGraph:
    def __init__(self):
        self.graph = self.build_graph()

    def build_graph(self) -> CompiledStateGraph:
        """
        Constructs and compiles a LangGraph instance.

        This function initializes a `SupervisorAgent` to create the base graph structure
        and uses an `InMemorySaver` as the checkpointer for the compilation process.

        The resulting compiled graph can be used to execute Supervisor workflow in LangGraph Studio.

        Returns:
        CompiledGraph: A fully compiled LangGraph instance ready for execution.
        """
        model = get_llm()

        get_farm_yields_tool = GetFarmYieldTool()

        get_farm_yields_tool = create_react_agent(
            model=model,
            tools=[get_farm_yields_tool],
            name="get_farm_yields",
        )
    
        fetch_harvest_tool = FetchHarvestTool(
            remote_agent_card=farm_agent_card,
        )

        fetch_harvest = create_react_agent(
            model=model,
            tools=[fetch_harvest_tool],
            name="fetch_harvest",
        )
        graph = create_supervisor(
            model=model,
            agents=[get_farm_yields_tool, fetch_harvest],  # worker agents list
        prompt=(
                "You are a supervisor agent responsible for handling coffee requests in pounds (lb).\n\n"

                "## TASK FLOW:\n"
                "1. First, use the tool `get_farm_yields` to retrieve a dictionary mapping farm names to their available coffee yields in pounds.\n"
                "2. From the returned dictionary, identify the farm with the **largest yield**.\n"
                "3. Then, assign the task to the tool `fetch_harvest` to send the user's request (in pounds) to that top-yield farm.\n"
                "   - Input: the selected farm name and the quantity requested by the user.\n\n"

                "## RULES:\n"
                "- Only proceed if the user's prompt clearly asks for coffee in pounds. Forgive them if they make small typos.\n"
                "- If the request is invalid or unrelated to coffee/lb, respond with:\n"
                "  \"I'm sorry, I can only help with coffee requests in pounds.\"\n"
                " Provide a detailed explanation of why the request is invalid.\n" # delete later
                "- If either tool returns an error or fails, return a user-friendly error message explaining the failure.\n"
                "- If both tool calls succeed, respond with a message in this format:\n"
                "  \"<FarmName> will fulfill your order of <X> lb of coffee. It will be sent to you shortly.\"\n"
            ),
            # add_handoff_back_messages=False,
            output_mode="last_message",
        ).compile()
        logger.debug("LangGraph supervisor created and compiled successfully.")
        return graph

    async def serve(self, prompt: str):
        """
        Processes the input prompt and returns a response from the graph.
        Args:
            prompt (str): The input prompt to be processed by the graph.
        Returns:
            str: The response generated by the graph based on the input prompt.
        """
        try:
            logger.debug(f"Received prompt: {prompt}")
            if not isinstance(prompt, str) or not prompt.strip():
                raise ValueError("Prompt must be a non-empty string.")
            result = await self.graph.ainvoke({
                "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
                ],
            }, {"configurable": {"thread_id": uuid.uuid4()}})

            logger.debug(f"Graph response: {result}")
            messages = result.get("messages", [])
            logger.debug(f"Graph response messages: {messages}")
            if not messages:
                raise RuntimeError("No messages found in the graph response.")

            # Find the last AIMessage with non-empty content
            for message in reversed(messages):
                if isinstance(message, AIMessage) and message.content.strip():
                    logger.debug(f"Valid AIMessage found: {message.content.strip()}")
                    return message.content.strip()

            raise RuntimeError("No valid AIMessage found in the graph response.")
        except ValueError as ve:
            logger.error(f"ValueError in serve method: {ve}")
            raise ValueError(str(ve))
        except Exception as e:
            logger.error(f"Error in serve method: {e}")
            raise Exception(str(e))
