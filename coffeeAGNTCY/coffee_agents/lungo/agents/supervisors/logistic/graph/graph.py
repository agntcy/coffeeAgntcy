# Copyright AGNTCY Contributors (https://github.com/agntcy)
# SPDX-License-Identifier: Apache-2.0

import logging
import uuid

from langchain_core.prompts import PromptTemplate
from langchain_core.messages import AIMessage
from langgraph.graph.state import CompiledStateGraph
from langgraph.graph import MessagesState
from langgraph.graph import StateGraph
from langgraph.prebuilt import ToolNode
from ioa_observe.sdk.decorators import agent, graph

from agents.supervisors.logistic.graph.tools import (
    create_order,
    next_tools_or_end
)
from common.llm import get_llm

logger = logging.getLogger("lungo.logistic.supervisor.graph")

class NodeStates:
    ORDERS = "orders_broker"
    ORDERS_TOOLS = "orders_tools"

class GraphState(MessagesState):
    """
    Represents the state of our graph, passed between nodes.
    """
    next_node: str

@agent(name="logistic_agent")
class LogisticGraph:
    def __init__(self):
        self.graph = self.build_graph()

    @graph(name="logistic_graph")
    def build_graph(self) -> CompiledStateGraph:
        """
        Constructs and compiles a LangGraph instance.

        Agent Flow:

        supervisor_agent
            - converse with user and coordinate app flow

        inventory_agent
            - get inventory for a specific farm or broadcast to all farms

        orders_agent
            - initiate orders with a specific farm and retrieve order status

        reflection_agent
            - determine if the user's request has been satisfied or if further action is needed

        Returns:
        CompiledGraph: A fully compiled LangGraph instance ready for execution.
        """

        self.orders_llm = None

        workflow = StateGraph(GraphState)

        # --- 1. Define Node States ---
        workflow.add_node(NodeStates.ORDERS, self._orders_node)
        workflow.add_node(NodeStates.ORDERS_TOOLS, ToolNode([create_order]))

        # --- 2. Define the Agentic Workflow ---
        workflow.set_entry_point(NodeStates.ORDERS)

        # Add conditional edges from the supervisor
        workflow.add_conditional_edges(NodeStates.ORDERS, next_tools_or_end)
        workflow.add_edge(NodeStates.ORDERS_TOOLS, NodeStates.ORDERS)

        return workflow.compile()


    async def _orders_node(self, state: GraphState) -> dict:
        if not self.orders_llm:
            self.orders_llm = get_llm().bind_tools([create_order])

        prompt = PromptTemplate(
            template=(
                "You are an orders broker for a global coffee exchange company. "
                "You handle user requests about placing and checking orders with coffee farms.\n\n"
                "Rules:\n"
                "1. Always call the create_order tool.\n"
                "2. If the user wants order status, retrieve or summarize it.\n"
                "3. Do not create a duplicate order for the same request.\n"
                "4. Ask for clarification only when required.\n"
                "5. FINAL DELIVERY HANDLING:\n"
                "   If any earlier tool or agent message contains the exact token 'DELIVERED' "
                "(indicates the order was fully delivered), DO NOT call tools again and DO NOT ask questions. "
                "Respond ONLY with a multiline plain text summary in the following format without any newline character (and nothing else):\n"
                "   Order <extract order id from tool execution result> from <farm (Title Case) or unknown> for <quantity or unknown> units at <price or unknown> has been successfully delivered."
                "   - Infer farm / quantity / price from prior messages; if missing use 'unknown'.\n"
                "   - Never call tools after 'DELIVERED' appears.\n\n"
                "Output:\n"
                "- Normal flow: helpful answer or tool call.\n"
                "- Delivery flow: ONLY the specified formatted text block.\n\n"
                "Conversation messages:\n"
                "{user_message}"
            ),
            input_variables=["user_message"]
        )

        chain = prompt | self.orders_llm

        llm_response = chain.invoke({
            "user_message": state["messages"],
        })

        if llm_response.tool_calls:
            logger.info(f"Tool calls detected from orders_node: {llm_response.tool_calls}")
            logger.debug(f"Messages: {state['messages']}")
        return {
            "messages": [llm_response]
        }

    async def serve(self, prompt: str):
        """
        Processes the input prompt and returns a response from the graph.
        Args:
            prompt (str): The input prompt to be processed by the graph.
        Returns:
            str: The response generated by the graph based on the input prompt.
        """
        try:
            logger.debug(f"Received prompt: {prompt}")
            if not isinstance(prompt, str) or not prompt.strip():
                raise ValueError("Prompt must be a non-empty string.")
            result = await self.graph.ainvoke({
                "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
                ],
            }, {"configurable": {"thread_id": uuid.uuid4()}})

            messages = result.get("messages", [])
            if not messages:
                raise RuntimeError("No messages found in the graph response.")

            # Find the last AIMessage with non-empty content
            for message in reversed(messages):
                if isinstance(message, AIMessage) and message.content.strip():
                    logger.debug(f"Valid AIMessage found: {message.content.strip()}")
                    return message.content.strip()

            raise RuntimeError("No valid AIMessage found in the graph response.")
        except ValueError as ve:
            logger.error(f"ValueError in serve method: {ve}")
            raise ValueError(str(ve))
        except Exception as e:
            logger.error(f"Error in serve method: {e}")
            raise Exception(str(e))
